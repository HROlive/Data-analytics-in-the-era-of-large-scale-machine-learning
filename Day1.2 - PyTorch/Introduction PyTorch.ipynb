{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"oxUBtTcD40KT"},"source":["# Introduction to Deep Learning with PyTorch\n","\n","\n","![AI_venn.png](https://drive.google.com/uc?export=view&id=1pThL5BY428RjG15vBI4dWdvf6fUtyXbw)\n","\n","\n","## Artificial Intelligence\n","AI is a very broad term and includes anything that enables computers to mimic human intelligence. On an elementary level, AI can be a predefined rule that enables a machine to react to specific situations in predetermined ways, or in simple terms a set of if-else rules. When we are talking about Artificial Intelligence it's worthwhile to concentrate on two important subfields.\n","\n","## Machine Learning\n","Machine learning is a subset of Artificial Intelligence where a **series of algorithms analyze data** and **learn from it to make informed decisions based on learned insights**.\n","The mathematical inception of a lot of the basis of machine learning is not new and has been around since the 60s. However, it hasn't been until relatively recently that we have the computational power to leverage it. The rapid progression of machine learning in the past years has lead such methods to be adopted in virtually every industry sector.\n","\n","ML incorporates a lot of classical algorithms for learning how to perform tasks from learned examples, such as clusterring, regression and classification. There are generally four types of machine learning:\n","* **Supervised** learning, where the model learns from labelled examples. \n","* **Unsupervised** learning, where data has no labels, i.e. the system does not know the correct answer. A popular example is classification problems, like identifying clusters of customers based on their attributes for marketing campaigns.\n","* **Semi-supervised** learning is a combination of the two above, i.e. it uses both labelled and unlabelled data for training. It can be useful when we have a mixture of labelled and unlabelled data, where the cost of labelling data is too high.\n","* **Reinforcement** learning, where the model learns by itself which actions yield the highest rewards trough trial and error. \n","\n","## Deep Learning\n","A subset of Machine Learning based on the use of **neural networks**, even though this term is often used interchangeably with Machine Learning. The term **deep** denotes the use of a neural network with 3 or more layers, including the **input** and **output** layers, i.e. one more **hidden layers**. \n","\n","![neural_network](https://drive.google.com/uc?export=view&id=1LbAhOtBInXdTzV7AMPEMAUqIT1ybH9hC)\n","\n","Neural networks is not a new concept, but have been making large strides in the past decade due to the vast amount of data available and most importantly the use of GPUs or other accelerate units (like TPUs). "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"y_Z2Df7CIpSZ"},"source":["# Introduction to PyTorch\n","![pytorch](https://drive.google.com/uc?export=view&id=1GP0ENtTIW_raHLoYzErL-mbSzZgkmzv1)\n","\n","<https://pytorch.org/>\n","\n","\n","PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. It was developed and published by META AI (Facebook) and has been amongst the most popular frameworks for the development of neural networks for the past years. \n","\n","**What is PyTorch?**\n","\n","PyTorch is a Python-based scientific computing package serving two broad purposes:\n","\n","* A replacement for NumPy to use the power of GPUs and other accelerators.\n","* An automatic differentiation library that is useful to implement neural networks.\n","\n","PyTorch uses a specialised data structure, similar to numpy's ndarrays, called **tensors**, to encode the inputs and outputs of a model as well as the model's parameters. A very significant difference compared to numpy's arrays is the ability of tensors to run on GPUs or other accelerators to accelerate computing."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5029,"status":"ok","timestamp":1684707166242,"user":{"displayName":"Pantelis Georgiades","userId":"10383021368498867049"},"user_tz":-180},"id":"A79_ExuLH_wt"},"outputs":[],"source":["# Import the pytorch and numpy libraries\n","import torch\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"Zma2l76l40KV"},"source":["Check whether CUDA is available and print out the GPU model you are using."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wskqAZMqJWC4"},"outputs":[],"source":["print(f\"Is CUDA available in the session? {torch.cuda.is_available()}\")\n","print(f\"\\nHow many CUDA enabled devices are available? {torch.cuda.device_count()}\")\n","print(f\"\\nWhich CUDA device is available? {torch.cuda.get_device_name(0)}\")"]},{"cell_type":"markdown","metadata":{"id":"nZcZcZJY40KX"},"source":["## PyTorch tensors tutorial\n","\n","There are various ways to initialise a tensor.\n","\n","**Directly from data**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1684707166245,"user":{"displayName":"Pantelis Georgiades","userId":"10383021368498867049"},"user_tz":-180},"id":"247vaCyA40KY"},"outputs":[],"source":["# 2x2 list\n","data = [[1, 2], [3, 4]]\n","data_tensor = torch.tensor(data)"]},{"cell_type":"markdown","metadata":{"id":"tnQmwW8Y40KY"},"source":["**From a numpy array**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1684707166245,"user":{"displayName":"Pantelis Georgiades","userId":"10383021368498867049"},"user_tz":-180},"id":"INHBbQyw40KZ"},"outputs":[],"source":["# Convert the list to a numpy array\n","data_numpy = np.array(data)\n","data_np_tensor = torch.tensor(data_numpy)"]},{"cell_type":"markdown","metadata":{"id":"P1cZcfyH40KZ"},"source":["**From another tensor**\n","\n","We can create a new tensor from an existing one, which will retain the properties of the argument one (shape, datatype), unless we explicitely override any of the properties."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UYU1tGzU40Ka"},"outputs":[],"source":["# Create a new tensor with the same properties as data_tensor (shape and datatype), filled with 1s.\n","x_ones = torch.ones_like(data_tensor)\n","\n","# Create a new tensor from data_tensor, filled with random numbers and change the datatype to float.\n","x_rand = torch.rand_like(data_tensor, dtype=torch.float)\n","\n","print(f\"Argument tensor: \\n{data_tensor}\\n\")\n","print(f\"New tensor filled with 1s: \\n{x_ones}\\n\")\n","print(f\"New tensor filled with random numbers: \\n{x_rand}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"bM08chXH40Ka"},"source":["**We can also initialise the tensor by defining its shape**\n","\n","The *shape* argument is a *tuple* of tensor dimensions.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6VFDnQEA40Kb"},"outputs":[],"source":["# Define the shape\n","shape = (2, 3,)   # <- rows x columns\n","\n","# Tensor with random numbers\n","tensor_random = torch.rand(shape)\n","# Tensor filles with zeros\n","tensor_zeros = torch.zeros(shape)\n","\n","# Print the tensors\n","print(f\"Filled with random numbers: \\n{tensor_random}\\n\")\n","print(f\"Filled with zeros: \\n{tensor_zeros}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"Chwvm9qe40Kb"},"source":["**Tensor operations**\n","\n","There are over a hundred tensor operations, which you can find here <https://pytorch.org/docs/stable/torch.html>.\n","\n","To take advantage of the GPU acceleration of tensors, we should move them to the GPU (if available):"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4287,"status":"ok","timestamp":1684707170520,"user":{"displayName":"Pantelis Georgiades","userId":"10383021368498867049"},"user_tz":-180},"id":"qScWc3iW40Kb","outputId":"6a6687a1-e760-45a3-e8ff-7457fcdee3ef"},"outputs":[],"source":["# Check if the GPU is available and assign the proper name to the device variable\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Assign the device to the tensor (by default it's on cpu)\n","print(f\"Default tensor device assignment: {data_tensor.device}\")\n","data_tensor = data_tensor.to(device)\n","print(f\"New device: {data_tensor.device}\")"]},{"cell_type":"markdown","metadata":{"id":"1jtZB1WH40Kc"},"source":["**Indexing** a tensor works just like numpy arrays"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7AHB8ws40Kc"},"outputs":[],"source":["# Define a new tensor of shape 4x4 filled with ones\n","tensor = torch.ones((4, 4), dtype=torch.int64)\n","\n","# Assign the number 2 to all the members of the second column\n","tensor[:, 1] = 2\n","print(tensor)"]},{"cell_type":"markdown","metadata":{"id":"W_-lXVnw40Kc"},"source":["We can also concatenate tensors using the **torch.cat** function along a predefined axis, denoted by **dim**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7d9m37XZ40Kc"},"outputs":[],"source":["new_tensor = torch.cat([tensor, tensor, tensor], dim=1)  # concat along the x axis\n","new_tensor2 = torch.cat([tensor, tensor], axis=0)        # concat along the y axis\n","\n","print(f\"Along x axis:\\n{new_tensor}\\n\")\n","print(f\"Along y axis:\\n{new_tensor2}\")"]},{"cell_type":"markdown","metadata":{"id":"8pZWP4Zl40Kd"},"source":["## The **autograd** function\n","\n","One of the most important functions in PyTorch is the *torch.autograd* function. It's the automatic differentation engine that drives neural network training.\n","\n","Training a Neural Network happens in two stages: \n","\n","**Forward Propagation:** In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.\n","\n","**Backward Propagation:** In backprop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (gradients), and optimizing the parameters using gradient descent.\n","\n","### Differentiation in Autograd\n","\n","In this example we will look at how *autograd* collects gradients. To start with let's create two tensors, *a* and *b* with *requires_grad=True* (this enables autograd to track all the operations)."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1684707170521,"user":{"displayName":"Pantelis Georgiades","userId":"10383021368498867049"},"user_tz":-180},"id":"6914LCKI40Kd"},"outputs":[],"source":["a = torch.tensor([5., 7.], requires_grad=True)\n","b = torch.tensor([8., 4.], requires_grad=True)"]},{"cell_type":"markdown","metadata":{"id":"CrFoHdU540Kd"},"source":["Create another tensor *Q* from *a* and *b*.\n","$$\n","Q = 5a^{3} - b^{2}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684707170521,"user":{"displayName":"Pantelis Georgiades","userId":"10383021368498867049"},"user_tz":-180},"id":"49CR4f2-40Kd"},"outputs":[],"source":["Q = 5*a**3 - b**2"]},{"cell_type":"markdown","metadata":{"id":"jdviVkZU40Ke"},"source":["Assume that *a* and *b* are parameters of a neural network, and *Q* is the error. While training a neural network, we want gradients of the error w.r.t. parameters, i.e.\n","$$\n","\\frac{\\partial Q}{\\partial a}=15a^{2}\n","$$\n","\n","$$\n","\\frac{\\partial Q}{\\partial b}=-2b\n","$$\n","\n","*.backward()* calculates these gradients and stores them in the respective tensor *.grad* attribute.\n","\n","To do so, we need to pass a *gradient* argument in *Q.backward()*  because it is a vector. *gradient* is a tensor of the same shape as *Q*, and it represents the gradient of Q w.r.t itself, i.e.\n","$$\n","\\frac{dQ}{dQ}=1\n","$$\n","Equivalently, we can also aggregate *Q* into a scalar and call backward implicitly, like *Q.sum().backward()*."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684707170521,"user":{"displayName":"Pantelis Georgiades","userId":"10383021368498867049"},"user_tz":-180},"id":"0ipcMDrM40Ke"},"outputs":[],"source":["external_grad = torch.tensor([1., 1.])\n","Q.backward(gradient=external_grad)"]},{"cell_type":"markdown","metadata":{"id":"mpSZeriT40Ke"},"source":["Gradients are now stored in *a.grad* and *b.grad*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UIe4Meko40Ke"},"outputs":[],"source":["print(f\"a.grad: {a.grad}\\nb.grad: {b.grad}\\n\")\n","\n","# Manually check if this is correct\n","print(15*a**2 == a.grad)\n","print(-2*b == b.grad)"]},{"cell_type":"markdown","metadata":{"id":"_xr1i7hD40Ke"},"source":["------\n","## Let's make a neural network!\n","\n","The building blocks of neural networks in PyTorch are contained in the **torch.nn** package.\n","\n","*nn* depends on *autograd* to define models and differentiate them. An *nn.Module* contains layers, and a method *forward(input)* that returns the *output*. The *input* is the first layer of the neural network and the *output* the last.\n","\n","A typical training procedure for a neural network in PyTorch is as follows:\n","* The neural network that has some learnable parameters (or weights) is defined.\n","* We iterate over a dataset of inputs.\n","* The input is processed through the network.\n","* Loss is computed (the difference between the model's output and ground truth)\n","* Gradients are propagated back into the network's parameters\n","* The weights of the network are updated, typically using a simple update rule $ weight=weight - learning\\_rate * gradient $\n","\n","### Import the necessary packages"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":575,"status":"ok","timestamp":1684707171092,"user":{"displayName":"Pantelis Georgiades","userId":"10383021368498867049"},"user_tz":-180},"id":"ZO8E2hoc40Kf"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"rrjAQ3z3UA7d"},"source":["## Fashion-MNIST dataset\n","\n","In this example we will use the FashionMNIST dataset (<https://github.com/zalandoresearch/fashion-mnist>). \n","\n","![fashionMNIST](https://github.com/zalandoresearch/fashion-mnist/blob/master/doc/img/fashion-mnist-sprite.png?raw=true)\n","\n","It's comprised of a training set of 60000 examples and a test set of 10000 examples, from Zalando (e-commerce platform). Each example is a 28x28 grayscale image and an associated label fron one of 10 classes.\n","\n","| Label | Description |\n","| --- | --- |\n","| 0 | T-shirt/top |\n","| 1 | Trouser |\n","| 2 | Pullover |\n","| 3 | Dress |\n","| 4 | Coat |\n","| 5 | Sandal |\n","| 6 | Shirt |\n","| 7 | Sneaker |\n","| 8 | Bag |\n","| 9 | Ankle boot |\n","\n","<https://doi.org/10.48550/arXiv.1708.07747>\n","\n","The Fashion-MNIST dataset is available through the **datasets** module in **torchvision**.\n","\n","To load the dataset we define the following parameters:\n","* **root** is the path to where the train/test data are stored\n","* **train** specifies training or test dataset\n","* **download=True** specifies whether to download the data or not (if not available in root)\n","* **transform** and **target_transform** specify the feature and label transformations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QQMFR9ULgKNj"},"outputs":[],"source":["# First get the current working directory - We will use the same directory to store the data\n","wd = os.getcwd()\n","\n","# Download the training and test datasets\n","training_data = datasets.FashionMNIST(\n","    root=wd,\n","    train=True,\n","    download=True,\n","    transform=ToTensor()  # <- convert to tensor as neural networks don't know what to do with images\n",")\n","\n","test_data = datasets.FashionMNIST(\n","    root=wd,\n","    train=False,\n","    download=True,\n","    transform=ToTensor()\n",")"]},{"cell_type":"markdown","metadata":{"id":"IkBVhqtYhIwM"},"source":["## Visualise the Dataset\n","\n","We can index **Datasets** manually like a list: *training_data[index]* and then used matplotlib to visualise a number of examples from the training set."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"executionInfo":{"elapsed":649,"status":"ok","timestamp":1684707180988,"user":{"displayName":"Pantelis Georgiades","userId":"10383021368498867049"},"user_tz":-180},"id":"jzOOZZMIhiwj","outputId":"3ef9ff1b-db25-489a-9473-4a6634ef9ccd"},"outputs":[],"source":["# Define a dictionary with the labels\n","labels = {\n","    0: \"T-Shirt\",\n","    1: \"Trouser\",\n","    2: \"Pullover\",\n","    3: \"Dress\",\n","    4: \"Coat\",\n","    5: \"Sandal\",\n","    6: \"Shirt\",\n","    7: \"Sneaker\",\n","    8: \"Bag\",\n","    9: \"Ankle Boot\",\n","}\n","\n","# Visualise 5 of them using matplotlib\n","figure = plt.figure(figsize=(25, 5))\n","cols, rows = 5, 1\n","for i in range(1, cols*rows+1):\n","  # Choose a random index from the training_data dataset\n","  sample_idx = torch.randint(len(training_data), size=(1, )).item()\n","  # Get the image data and the label for this example\n","  img, label = training_data[sample_idx]\n","  figure.add_subplot(rows, cols, i)\n","  plt.title(labels[label])\n","  plt.axis(\"off\")\n","  plt.imshow(img.squeeze(), cmap=\"gray\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"BnBQNfIkjs-_"},"source":["To use data with PyTorch we must create a custom **Dataset** class, which must implement three functions\n","* \\__init__ : Runs once when instantiating the Dataset object.\n","* \\__len__ : Returns the number of examples in the dataset.\n","* \\__getitem__ : Loads and returns a sample from the dataset specified by a given idx."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1684707180988,"user":{"displayName":"Pantelis Georgiades","userId":"10383021368498867049"},"user_tz":-180},"id":"YlSqR-zhkha_"},"outputs":[],"source":["import pandas as pd\n","from torchvision.io import read_image\n","\n","\n","class CustomImageDataset(Dataset):\n","\n","  def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n","    self.img_labels = pd.read_csv(annotations_file)  # Labels are stored in a .csv which we read here using pandas\n","    self.img_dir = img_dir  # The directory where the images are stored\n","    self.transform = transform\n","    self.target_transform = target_transform\n","\n","  def __len__(self):\n","    return len(self.img_labels)\n","\n","  def __getitem__(self, idx):\n","    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])  # The first column of the pandas dataframe holds the filename for the images\n","    image = read_image(img_path)  # Use the torchvision function read_image to load the image\n","    label = self.img_labels.iloc[idx, 1]  # The second column of the dataframe holds the labels\n","    # If we pass a transform argument to the function it applies it here.\n","    if self.transform:\n","      image = self.transform(image)\n","    if self.target_transform:\n","      label = self.target_transform(label)\n","    return image, label"]},{"cell_type":"markdown","metadata":{"id":"rwEDn9Wvmanl"},"source":["## DataLoader module\n","\n","We used the **Dataset** module to retrieve our dataset's features and labels, but we now need to use the **DataLoader** module create the appropriate objects to pass to the neural network. **DataLoader** is an easy to use API, which abstracts a lot of behind the scenes operations that need to happen to use the data with neural networks (like creating \"minibatches\", shuffle the data at each epoch to limit overfitting etc.)\n","\n","We can iterate through the object, but since it's an iterable object we need to use **next(iter())**, i.e.\n","train_features, train_labels = next(iter(train_dataloader))\n","\n","This will return a mini-batch of size **batch_size** and if **shuffle** is enabled the mini batch is shuffled each time we call iter()."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1684707180990,"user":{"displayName":"Pantelis Georgiades","userId":"10383021368498867049"},"user_tz":-180},"id":"89VcUoGOm-Ka"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","\n","# Training data object (we enable shuffling of the data at each epoch and define the batch size)\n","train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)  \n","# Test data object\n","test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"qoGDf6sGn_gD"},"source":["## Define the neural network model\n","\n","As we mentioned above, in deep learning we use neural networks with 1 or more hidden layers. Let's build a fully-connected feed-forward neural network to tackle our classification problem. We will start simple, with two hidden layers, each containing 512 units.\n","\n","PyTorch provides **torch.nn**, which provides all the building blocks we need to build our neural network. To define our neural network we need to define a **class**, which subclasses **nn.Module** and initialise the network's layers in \\__init__. Every **nn.Module** subclass implements the operations of input data in the **forward** method."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1684707180990,"user":{"displayName":"Pantelis Georgiades","userId":"10383021368498867049"},"user_tz":-180},"id":"PDSGTuLKqyjy"},"outputs":[],"source":["from torch import nn\n","\n","\n","class NeuralNetwork(nn.Module):\n","  \n","  def __init__(self):\n","    super().__init__()  # <- This allows us to access methods and properties of a parent class (i.e. nn.Module)\n","    # Define the layers of the neural network here\n","    self.flatten = nn.Flatten()  # Arrange the data in a vector to feed to the neural network\n","    # Define a Sequence of layers (2 layers with 512 units each with a ReLU activation function)\n","    self.linear_relu_stack = nn.Sequential(\n","        nn.Linear(in_features=28*28, out_features=512),  # First argument in the Linear layer is the input size and second argument is the output size\n","        nn.ReLU(),\n","        nn.Linear(in_features=512, out_features=512),\n","        nn.ReLU(),\n","        nn.Linear(in_features=512, out_features=10)\n","    )\n","    \n","  # Next we need to define the forward() method, which implements the data flow through the layers\n","  # X is the input data\n","  def forward(self, X):\n","    x = self.flatten(X)\n","    logits = self.linear_relu_stack(x)\n","    return logits"]},{"cell_type":"markdown","metadata":{"id":"9J-IaE1wXDgz"},"source":["Next, we instantiate the neural network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Njym4mL_XDgz"},"outputs":[],"source":["model = NeuralNetwork()\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"p53Yl3n6XDgz"},"source":["Before we start training the model, we need to define a few **hyperparameters**, that is adjustable parameters that control the optimisation process and can impact the final performance and convergence rate of the model (more here <https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html>).\n","\n","In this example we will define the following:\n","* **Number of Epochs** - the number of times to iterate over the dataset.\n","* **Batch Size** - the number of data samples propagated through the network before the parameters are updated (usually limited by the amount of VRAM available).\n","* **Learning Rate** - How much to update model's parameters at each batch/epoch. This is trade off between learning speed and accuracy, higher values of learning rate can lead to faster convergence but achieve lower performance or lead to unpredictable behaviour during training (e.g. exploding gradients).\n","\n","### Loss function\n","\n","We also need to define the loss function, which measures the degree of similarity on the model's output compared to the target value, and it is what we are trying to minimise during the training procedures. In this case we will use the **Cross Entropy Loss** function, a common metric used to measure the performance of classification models. This will normalise the logits (model output) and compute the prediction error.\n","\n","### Optimizer\n","\n","The optimiser defines how the process of adjusting the model parameters to reduce model error in each training step is adjusted. In this case we will use the **Stochastic Gradient Descent** algorithm. Another popular algorithm is **Adam** and its variations (more here <https://pytorch.org/docs/stable/optim.html>).\n","\n","When initialising the optimizer, we need to register our model's parameters that need to be trained and pass the learning rate to it."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1684707180990,"user":{"displayName":"Pantelis Georgiades","userId":"10383021368498867049"},"user_tz":-180},"id":"NhP9hdFNXDg1"},"outputs":[],"source":["# Hyperparameters\n","learning_rate = 1e-3\n","batch_size=64\n","epochs=5\n","\n","# Loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Optimizer\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"VP5dJEy9XDg1"},"source":["Finally, we define the **train_loop** which loops over the optimization procedures and **test_loop** which evaluates the model's performance against the test data.\n","\n","Since the **dataloader** runs exclusively on cpu (in parallel with the multiprocessing backend) we need to move each batch to the GPU if we are using the GPU. "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1684707180990,"user":{"displayName":"Pantelis Georgiades","userId":"10383021368498867049"},"user_tz":-180},"id":"AXBfwsclXDg1"},"outputs":[],"source":["def train_loop(dataloader, model, loss_fn, optimizer, device=\"cpu\"):\n","    # Get the number of mini-batches\n","    size = len(dataloader.dataset)\n","    \n","    # Loop through the mini batches and perform the training procecures\n","    for batch, (X, y) in enumerate(dataloader):  # <- enumerate() is a great function to put in your arsenal if you haven't already!!\n","        \n","        # Move these to GPU if device is cuda\n","        if device == \"cuda\":\n","            X, y = X.cuda(), y.cuda()\n","        \n","        # Compute prediction and loss\n","        pred = model(X)\n","        loss = loss_fn(pred, y)\n","        \n","        # Perform the backpropagation procedure (update the parameters)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if batch % 200 == 0:\n","            loss, current = loss.item(), (batch+1)*len(X)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","            \n","\n","def test_loop(dataloader, model, loss_fn, device=\"cpu\"):\n","    \n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    test_loss, correct = 0, 0\n","    \n","    # We need to use the torch.no_grad() option here, as we don't want to update the\n","    # gradients of the network's parameters, just use it to calculate the loss\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            \n","            # Move these to GPU if device is cuda\n","            if device == \"cuda\":\n","                X, y = X.cuda(), y.cuda()\n","                \n","            # Calculate the model's prediction\n","            pred = model(X)\n","            # Get the loss of this batch and add it to the total\n","            test_loss += loss_fn(pred, y)\n","            # Get the number of correct predictions from the batch and add it to the total\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","            \n","    test_loss /= num_batches\n","    correct /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"gYOKYCS9XDg1"},"source":["Next we loop through the train_loop and test_loop for the number of epochs we defined to optimize the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eq69PjDuXDg1"},"outputs":[],"source":["%%time\n","\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model, loss_fn, optimizer)\n","    test_loop(test_dataloader, model, loss_fn)\n","    \n","print(\"Done!!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HtPq-oobXDg2"},"outputs":[],"source":["%%time\n","\n","model_gpu = NeuralNetwork()\n","model_gpu.to(\"cuda\")\n","\n","# Since we defined a new model we need to instantiate a new optimiser\n","optimizer = torch.optim.SGD(model_gpu.parameters(), lr=learning_rate)\n","\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model_gpu, loss_fn, optimizer, \n","               device=\"cuda\")\n","    test_loop(test_dataloader, model_gpu, loss_fn,\n","              device=\"cuda\")\n","    \n","print(\"Done!!\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eTjDtODJXDg2"},"source":["## Convolutional Neural Networks\n","\n","Now let's try a *Convolutional Neural Network*, which generally outperform fully connected feed-forward networks for computer vision tasks. A convolution is a mathematical operation used to extract features from an image and is defined by an image kernel (a small matrix). \n","\n","![conv](https://miro.medium.com/v2/resize:fit:640/1*ZCjPUFrB6eHPRi4eyP6aaA.gif)\n","\n","### Padding\n","\n","To avoid shrinking of the image size, one can use **padding**, that is add pixels with 0 value around the image.\n","\n","![padding](https://miro.medium.com/v2/resize:fit:640/format:webp/1*5rLRx19ot0QggMn9teY14Q.png)\n","\n","### Stride\n","\n","Sometimes is useful to implement the convolution kernel with step sizes larger than 1 pixel, to improve computation. This is called **stride**.\n","\n","![stride](https://miro.medium.com/v2/resize:fit:720/format:webp/1*y3Ydr1oCHRfOegWxZITIOA.png)\n","\n","### Convolution layer size\n","\n","If a $ n*n $ matrix is convolved with a $ f*f $ matrix with padding *p* and stride *s*, the output dimension is of size\n","$$\n","( \\frac{n+2p-f+1}{s} +1 ) * ( \\frac{n+2p-f+1}{s} +1 )\n","$$\n","\n","### Pooling\n","\n","In pooling layers we progressively reduce the spatial size of the representation to reduce the network complexity and computational cost. The most popular pooling layers are the **Max Poolint** and the **Average Pooling** layers.\n","\n","<sub> More at <https://medium.com/analytics-vidhya/convolution-padding-stride-and-pooling-in-cnn-13dc1f3ada26> </sub>\n","\n","### Batch Normalisation layer\n","\n","**Batch Normalisation** is a method used to make training a neural network more stable and efficient by normalising the layer's inputs by re-centering and re-scaling them. \n","\n","### Dropout\n","\n","During training, randomly zeroes some of the elements of the input tensor with probability *p* using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.\n","\n","This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper Improving neural networks by preventing co-adaptation of feature detectors.\n","\n","**NOTE**: Be careful when using both batch normalisation and dropout together, it might lead to unexpected results. \n","\n","## CNN architecture\n","\n","The CNN comprises of:\n","* A sequential layer with a kernel size of 3\\*3, with padding=1 and stride=1, followed by a *Batch normalization* layer, a ReLU activation function and a *Max pooling layer* with kernel size=2 and stride=2.\n","* A 2nd sequential layer with a kernel size of 3\\*3, padding=0 and stride=1, followed by a *Batch normalization* layer, a ReLU activation function and a *Max pooling layer* with kernel size=2 and stride=2.\n","* This is then passed to twp fully connected layers, the first with 600 units, followed by a dropout with p=0.25, a 2nd fully connected layer with 120 units and finally the output layer with 10 units (i.e. 10 classes).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684707310299,"user":{"displayName":"Pantelis Georgiades","userId":"10383021368498867049"},"user_tz":-180},"id":"gtU_Ls8gXDg2"},"outputs":[],"source":["class ConvNet(nn.Module):\n","    \n","    def __init__(self):\n","        \n","        super(ConvNet, self).__init__()\n","        \n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(num_features=32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        \n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        \n","        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n","        self.drop = nn.Dropout(0.25)\n","        self.fc2 = nn.Linear(in_features=600, out_features=120)\n","        self.fc3 = nn.Linear(in_features=120, out_features=10)\n","        \n","    def forward(self, X):\n","        \n","        out = self.layer1(X)\n","        out = self.layer2(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.fc1(out)\n","        out = self.drop(out)\n","        out = self.fc2(out)\n","        out = self.fc3(out)\n","        \n","        return out\n","        "]},{"cell_type":"markdown","metadata":{"id":"9p_-3W-fXDg2"},"source":["Instantiate the model and train it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mno5juh8XDg2"},"outputs":[],"source":["%%time\n","\n","model_conv = ConvNet()\n","\n","# Hyperparameters\n","learning_rate = 1e-3\n","epochs = 5\n","\n","# Optimizer\n","optimizer = torch.optim.SGD(model_conv.parameters(), lr=learning_rate)\n","\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model_conv, loss_fn, optimizer, \n","               device=\"cpu\")\n","    test_loop(test_dataloader, model_conv, loss_fn,\n","              device=\"cpu\")\n","    \n","print(\"Done!!\")"]},{"cell_type":"markdown","metadata":{"id":"w3CZ0nlHXDg2"},"source":["Now let's try it on the GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sz8duf9SXDg3"},"outputs":[],"source":["%%time\n","\n","device = \"cuda\"\n","\n","model_conv_gpu = ConvNet()\n","model_conv_gpu.to(device)\n","\n","# The convolutional neural network is much larger than the fully-connected one, so\n","# we might need to decrease the batch_size in order to be able to train it\n","batch_conv = 64\n","train_dataloader = DataLoader(training_data, batch_size=batch_conv, shuffle=True)  \n","test_dataloader = DataLoader(test_data, batch_size=batch_conv, shuffle=True)\n","\n","# Hyperparameters\n","learning_rate = 1e-3\n","epochs = 5\n","\n","# Optimizer\n","optimizer = torch.optim.SGD(model_conv_gpu.parameters(), lr=learning_rate)\n","\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model_conv_gpu, loss_fn, optimizer, \n","               device=device)\n","    test_loop(test_dataloader, model_conv_gpu, loss_fn,\n","              device=device)\n","    \n","print(\"Done!!\")"]},{"cell_type":"markdown","metadata":{"id":"iw52oOFuW1EL"},"source":["## Exercises\n","\n","1. Increase the batch size of the train and test sets. How does this affect training time and the accuracy acquired in 5 epochs? Can you think on what limits the batch size we can set for our training procedures?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12e0n_R1XP6_"},"outputs":[],"source":["%%time\n","\n","batch_size=64\n","\n","# Instantiate the model and send it to the GPU\n","model_conv_gpu = ConvNet()\n","model_conv_gpu.to(\"cuda\")\n","\n","# Train and test data loader objects\n","train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)  \n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n","\n","# Hyperparameters\n","learning_rate = 1e-3\n","epochs = 5\n","\n","# Optimizer\n","optimizer = torch.optim.SGD(model_conv_gpu.parameters(), lr=learning_rate)\n","\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model_conv_gpu, loss_fn, optimizer, \n","               device=device)\n","    test_loop(test_dataloader, model_conv_gpu, loss_fn,\n","              device=device)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["2. Increase or decrease the learning rate. How does this affect training? How do you think this affects the performance of the model wrt the number of epochs?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","\n","batch_size=64\n","\n","# Instantiate the model and send it to the GPU\n","model_conv_gpu = ConvNet()\n","model_conv_gpu.to(\"cuda\")\n","\n","# Train and test data loader objects\n","train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)  \n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n","\n","# Hyperparameters\n","learning_rate = 1e-3\n","epochs = 5\n","\n","# Optimizer\n","optimizer = torch.optim.SGD(model_conv_gpu.parameters(), lr=learning_rate)\n","\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model_conv_gpu, loss_fn, optimizer, \n","               device=device)\n","    test_loop(test_dataloader, model_conv_gpu, loss_fn,\n","              device=device)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CkW8QEjBb8GV"},"source":["3. Add another convolution layer to the network and run the training procedures. e.g. A third convolution layer which takes as input the output of the 2nd (64 channels) and outputs 128 with a MaxPool layer with kernel_size=2 and stride=2.\n","\n","**Remember** this to calculate the input and output sizes\n","\n","If a $ n*n $ matrix is convolved with a $ f*f $ matrix with padding *p* and stride *s*, the output dimension is of size\n","$$\n","( \\frac{n+2p-f+1}{s} +1 ) * ( \\frac{n+2p-f+1}{s} +1 )\n","$$\n","\n","> How do you think adding complexity to the architecture of the neural network affects training?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_00I4kDcL-t"},"outputs":[],"source":["class ConvNet2(nn.Module):\n","    \n","    def __init__(self):\n","        \n","        super(ConvNet2, self).__init__()\n","        \n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(num_features=32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        \n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        \n","        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n","        self.drop = nn.Dropout(0.25)\n","        self.fc2 = nn.Linear(in_features=600, out_features=120)\n","        self.fc3 = nn.Linear(in_features=120, out_features=10)\n","        \n","    def forward(self, X):\n","        \n","        out = self.layer1(X)\n","        out = self.layer2(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.fc1(out)\n","        out = self.drop(out)\n","        out = self.fc2(out)\n","        out = self.fc3(out)\n","        \n","        return out\n","batch_size=64\n","\n","# Instantiate the model and send it to the GPU\n","model_conv2 = ConvNet2()\n","model_conv2.to(\"cuda\")\n","\n","# Train and test data loader objects\n","train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)  \n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n","\n","# Hyperparameters\n","learning_rate = 1e-3\n","epochs = 5\n","\n","# Optimizer\n","optimizer = torch.optim.SGD(model_conv2.parameters(), lr=learning_rate)\n","\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model_conv2, loss_fn, optimizer, \n","               device=device)\n","    test_loop(test_dataloader, model_conv2, loss_fn,\n","              device=device)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
